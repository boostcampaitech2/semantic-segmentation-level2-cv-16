{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#하이퍼파라미터-세팅-및-seed-고정\" data-toc-modified-id=\"하이퍼파라미터-세팅-및-seed-고정-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>하이퍼파라미터 세팅 및 seed 고정</a></span></li><li><span><a href=\"#학습-데이터-EDA\" data-toc-modified-id=\"학습-데이터-EDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>학습 데이터 EDA</a></span></li><li><span><a href=\"#데이터-전처리-함수-정의-(Dataset)\" data-toc-modified-id=\"데이터-전처리-함수-정의-(Dataset)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>데이터 전처리 함수 정의 (Dataset)</a></span></li><li><span><a href=\"#Dataset-정의-및-DataLoader-할당\" data-toc-modified-id=\"Dataset-정의-및-DataLoader-할당-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset 정의 및 DataLoader 할당</a></span><ul class=\"toc-item\"><li><span><a href=\"#데이터-샘플-시각화-(Show-example-image-and-mask)\" data-toc-modified-id=\"데이터-샘플-시각화-(Show-example-image-and-mask)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>데이터 샘플 시각화 (Show example image and mask)</a></span></li></ul></li><li><span><a href=\"#baseline-model\" data-toc-modified-id=\"baseline-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>baseline model</a></span><ul class=\"toc-item\"><li><span><a href=\"#models.segmentation.fcn_resnet50\" data-toc-modified-id=\"models.segmentation.fcn_resnet50-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>models.segmentation.fcn_resnet50</a></span></li></ul></li><li><span><a href=\"#train,-validation,-test-함수-정의\" data-toc-modified-id=\"train,-validation,-test-함수-정의-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>train, validation, test 함수 정의</a></span></li><li><span><a href=\"#모델-저장-함수-정의\" data-toc-modified-id=\"모델-저장-함수-정의-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>모델 저장 함수 정의</a></span></li><li><span><a href=\"#모델-생성-및-Loss-function,-Optimizer-정의\" data-toc-modified-id=\"모델-생성-및-Loss-function,-Optimizer-정의-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>모델 생성 및 Loss function, Optimizer 정의</a></span></li><li><span><a href=\"#저장된-model-불러오기-(학습된-이후)\" data-toc-modified-id=\"저장된-model-불러오기-(학습된-이후)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>저장된 model 불러오기 (학습된 이후)</a></span><ul class=\"toc-item\"><li><span><a href=\"#plot_examples()-시각화-함수-정의\" data-toc-modified-id=\"plot_examples()-시각화-함수-정의-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span><code>plot_examples()</code> 시각화 함수 정의</a></span><ul class=\"toc-item\"><li><span><a href=\"#train-set-시각화\" data-toc-modified-id=\"train-set-시각화-9.1.1\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>train set 시각화</a></span></li><li><span><a href=\"#validation-set-시각화\" data-toc-modified-id=\"validation-set-시각화-9.1.2\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>validation set 시각화</a></span></li><li><span><a href=\"#test-set-시각화\" data-toc-modified-id=\"test-set-시각화-9.1.3\"><span class=\"toc-item-num\">9.1.3&nbsp;&nbsp;</span>test set 시각화</a></span></li></ul></li></ul></li><li><span><a href=\"#submission을-위한-test-함수-정의\" data-toc-modified-id=\"submission을-위한-test-함수-정의-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>submission을 위한 test 함수 정의</a></span></li><li><span><a href=\"#submission.csv-생성\" data-toc-modified-id=\"submission.csv-생성-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>submission.csv 생성</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:54:58.701660Z",
     "start_time": "2021-10-04T05:54:58.680659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.7.1\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-SXM2-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score, add_hist\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# GPU 사용 가능 여부에 따라 device 정보 저장\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:54:59.151185Z",
     "start_time": "2021-10-04T05:54:59.135661Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8  # Mini-batch size\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:54:59.321661Z",
     "start_time": "2021-10-04T05:54:59.310161Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:04.002687Z",
     "start_time": "2021-10-04T05:54:59.781190Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "dataset_path  = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train_all.json'\n",
    "\n",
    "# # Read annotations\n",
    "# with open(anns_file_path, 'r') as f:\n",
    "#     dataset = json.loads(f.read())\n",
    "\n",
    "# categories = dataset['categories']\n",
    "# anns = dataset['annotations']\n",
    "# imgs = dataset['images']\n",
    "# nr_cats = len(categories)\n",
    "# nr_annotations = len(anns)\n",
    "# nr_images = len(imgs)\n",
    "\n",
    "# # Load categories and super categories\n",
    "# cat_names = []\n",
    "# super_cat_names = []\n",
    "# super_cat_ids = {}\n",
    "# super_cat_last_name = ''\n",
    "# nr_super_cats = 0\n",
    "# for cat_it in categories:\n",
    "#     cat_names.append(cat_it['name'])\n",
    "#     super_cat_name = cat_it['supercategory']\n",
    "#     # Adding new supercat\n",
    "#     if super_cat_name != super_cat_last_name:\n",
    "#         super_cat_names.append(super_cat_name)\n",
    "#         super_cat_ids[super_cat_name] = nr_super_cats\n",
    "#         super_cat_last_name = super_cat_name\n",
    "#         nr_super_cats += 1\n",
    "\n",
    "# print('Number of super categories:', nr_super_cats)\n",
    "# print('Number of categories:', nr_cats)\n",
    "# print('Number of annotations:', nr_annotations)\n",
    "# print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [\n",
    "                    'Backgroud',\n",
    "                    'General trash',\n",
    "                    'Paper',\n",
    "                    'Paper pack',\n",
    "                    'Metal',\n",
    "                    'Glass',\n",
    "                    'Plastic',\n",
    "                    'Styrofoam',\n",
    "                    'Plastic bag',\n",
    "                    'Battery',\n",
    "                    'Clothing'\n",
    "                    ]\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # General trash = 1, ... , Cigarette = 10\n",
    "            anns = sorted(anns, key=lambda idx : len(idx['segmentation'][0]), reverse=False)\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks[self.coco.annToMask(anns[i]) == 1] = pixel_value\n",
    "            masks = masks.astype(np.int8)\n",
    "                        \n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            return images, image_infos\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:11.389706Z",
     "start_time": "2021-10-04T06:16:07.146708Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.97s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           pin_memory=True,\n",
    "                                           drop_last=True,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=True,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 샘플 시각화 (Show example image and mask)\n",
    "\n",
    "- `train_loader` \n",
    "- `val_loader` \n",
    "- `test_loader` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:55:16.433698Z",
     "start_time": "2021-10-04T05:55:16.425186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_colormap = pd.read_csv(\"class_dict.csv\")\n",
    "class_labels = {\n",
    "0 :'Backgroud',\n",
    "1 : \"General trash\",\n",
    "2: \"Paper\",\n",
    "3:\"Paper pack\",\n",
    "4:\"Metal\",\n",
    "5:\"Glass\",\n",
    "6:\"Plastic\",\n",
    "7:\"Styrofoam\",\n",
    "8:\"Plastic bag\",\n",
    "9:\"Battery\",\n",
    "10:\"Clothing\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model\n",
    "\n",
    "### models.segmentation.fcn_resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:11:45.167160Z",
     "start_time": "2021-10-04T05:11:44.782163Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model_name = 'Effb4_DLV3+'\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b4\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=11,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:11:48.065691Z",
     "start_time": "2021-10-04T05:11:45.412662Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "# x = torch.randn([2, 3, 512, 512])\n",
    "# print(f\"input shape : {x.shape}\")\n",
    "# out = model(x)\n",
    "# print(f\"output shape : {out.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:59.368955Z",
     "start_time": "2021-09-08T08:27:59.351957Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    wandb.init(\n",
    "                project=\"segmentation\", \n",
    "                entity=\"passion-ate\",\n",
    "                name='7_'+f'{model_name}')\n",
    "    print(f'Start training..')\n",
    "    n_class = 11\n",
    "    best_miou = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        hist = np.zeros((n_class, n_class))\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       \n",
    "            masks = torch.stack(masks).long() \n",
    "\n",
    "            \n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # device 할당\n",
    "            model = model.to(device)\n",
    "\n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks, outputs, n_class=n_class)\n",
    "            acc, acc_cls, mIoU, fwavacc, IoU = label_accuracy_score(hist)\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_loader)}], \\\n",
    "                        Loss: {round(loss.item(),4)}, mIoU: {round(mIoU,4)}')\n",
    "                wandb.log({\n",
    "                    \"train/mIoU\" : round(mIoU,4),\n",
    "                    \"train/loss\" : round(loss.item(),4)\n",
    "                })\n",
    "            \n",
    "             \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            miou = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if miou > best_miou:\n",
    "                print(f\"Best performance at epoch: {epoch + 1}\")\n",
    "                print(f\"Save model in {saved_dir}\")\n",
    "                best_miou = miou\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:59.631310Z",
     "start_time": "2021-09-08T08:27:59.620809Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print(f'Start validation #{epoch}')\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_class = 11\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        \n",
    "        hist = np.zeros((n_class, n_class))\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       \n",
    "            masks = torch.stack(masks).long()  \n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "            \n",
    "            # device 할당\n",
    "            model = model.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            masks = masks.detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks, outputs, n_class=n_class)\n",
    "            if step % log_every == 0:\n",
    "                wandb.log(\n",
    "                {\"my_image_key\" : wandb.Image(images[0, :, :, :], masks={\n",
    "                    \"predictions\" : {\n",
    "                        \"mask_data\" : outputs[0, :, :],\n",
    "                        \"class_labels\" : class_labels\n",
    "                    },\n",
    "                    \"ground_truth\" : {\n",
    "                        \"mask_data\" : masks[0, :, :],\n",
    "                        \"class_labels\" : class_labels\n",
    "                    }\n",
    "                })\n",
    "                })\n",
    "        \n",
    "        acc, acc_cls, mIoU, fwavacc, IoU = label_accuracy_score(hist)\n",
    "        IoU_by_class = [{classes : round(IoU,4)} for IoU, classes in zip(IoU , category_names)]\n",
    "        \n",
    "        avrg_loss = total_loss / cnt\n",
    "        log = {\n",
    "                    \"val/mIoU\" : mIoU,\n",
    "                    \"val/loss\" : avrg_loss,\n",
    "                    \"val/accuracy\" : acc,\n",
    "                }\n",
    "        for d in IoU_by_class:\n",
    "            for cls in d:\n",
    "                log[f'val/{cls}_IoU'] = d[cls]\n",
    "        wandb.log(log)\n",
    "        print(f'Validation #{epoch}  Average Loss: {round(avrg_loss.item(), 4)}, Accuracy : {round(acc, 4)}, \\\n",
    "                mIoU: {round(mIoU, 4)}')\n",
    "        print(f'IoU by class : {IoU_by_class}')\n",
    "        \n",
    "        \n",
    "    return mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:31.592162Z",
     "start_time": "2021-10-04T05:17:31.576161Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1\n",
    "log_every = 10\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "\n",
    "def save_model(model, saved_dir, file_name='DLv3_res101_best_model(pretrained).pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:32.708161Z",
     "start_time": "2021-10-04T05:17:32.695660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:13:57.428660Z",
     "start_time": "2021-10-04T05:13:57.419159Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1dv3n2ve) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3059... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9abdf0d1b7d4454868b3645882a69c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 3.64MB of 3.64MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>█▅▄▃▁▂▁▂▂▂▁▂▁</td></tr><tr><td>train/mIoU</td><td>▁▃▄▄▅▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>0.4471</td></tr><tr><td>train/mIoU</td><td>0.2041</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 27 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">7_Effb4_DLV3+</strong>: <a href=\"https://wandb.ai/passion-ate/segmentation/runs/1dv3n2ve\" target=\"_blank\">https://wandb.ai/passion-ate/segmentation/runs/1dv3n2ve</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211019_111252-1dv3n2ve/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1dv3n2ve). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/passion-ate/segmentation/runs/syk3fszg\" target=\"_blank\">7_Effb4_DLV3+</a></strong> to <a href=\"https://wandb.ai/passion-ate/segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/20], Step [25/327],                         Loss: 1.4582, mIoU: 0.0977\n",
      "Epoch [1/20], Step [50/327],                         Loss: 1.0705, mIoU: 0.1248\n",
      "Epoch [1/20], Step [75/327],                         Loss: 0.9082, mIoU: 0.1353\n",
      "Epoch [1/20], Step [100/327],                         Loss: 0.9176, mIoU: 0.1428\n",
      "Epoch [1/20], Step [125/327],                         Loss: 0.7562, mIoU: 0.1502\n",
      "Epoch [1/20], Step [150/327],                         Loss: 0.6965, mIoU: 0.1587\n",
      "Epoch [1/20], Step [175/327],                         Loss: 0.4936, mIoU: 0.1655\n",
      "Epoch [1/20], Step [200/327],                         Loss: 0.4954, mIoU: 0.1735\n",
      "Epoch [1/20], Step [225/327],                         Loss: 0.5312, mIoU: 0.18\n",
      "Epoch [1/20], Step [250/327],                         Loss: 0.5274, mIoU: 0.1843\n",
      "Epoch [1/20], Step [275/327],                         Loss: 0.515, mIoU: 0.1918\n",
      "Epoch [1/20], Step [300/327],                         Loss: 0.567, mIoU: 0.1971\n",
      "Epoch [1/20], Step [325/327],                         Loss: 0.3256, mIoU: 0.2047\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.4109, Accuracy : 0.8866,                 mIoU: 0.313\n",
      "IoU by class : [{'Backgroud': 0.9231}, {'General trash': 0.2986}, {'Paper': 0.6559}, {'Paper pack': 0.0025}, {'Metal': 0.0}, {'Glass': 0.0001}, {'Plastic': 0.2953}, {'Styrofoam': 0.5283}, {'Plastic bag': 0.7395}, {'Battery': 0.0}, {'Clothing': 0.0}]\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n",
      "Epoch [2/20], Step [25/327],                         Loss: 0.3978, mIoU: 0.2924\n",
      "Epoch [2/20], Step [50/327],                         Loss: 0.2746, mIoU: 0.3124\n",
      "Epoch [2/20], Step [75/327],                         Loss: 0.3799, mIoU: 0.3121\n",
      "Epoch [2/20], Step [100/327],                         Loss: 0.3461, mIoU: 0.3158\n",
      "Epoch [2/20], Step [125/327],                         Loss: 0.3281, mIoU: 0.3142\n",
      "Epoch [2/20], Step [150/327],                         Loss: 0.334, mIoU: 0.317\n",
      "Epoch [2/20], Step [175/327],                         Loss: 0.4579, mIoU: 0.319\n",
      "Epoch [2/20], Step [200/327],                         Loss: 0.4194, mIoU: 0.3273\n",
      "Epoch [2/20], Step [225/327],                         Loss: 0.7414, mIoU: 0.333\n",
      "Epoch [2/20], Step [250/327],                         Loss: 0.4733, mIoU: 0.3308\n",
      "Epoch [2/20], Step [275/327],                         Loss: 0.2688, mIoU: 0.3328\n",
      "Epoch [2/20], Step [300/327],                         Loss: 0.7067, mIoU: 0.3354\n",
      "Epoch [2/20], Step [325/327],                         Loss: 0.2779, mIoU: 0.3388\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 0.3158, Accuracy : 0.9043,                 mIoU: 0.4269\n",
      "IoU by class : [{'Backgroud': 0.9333}, {'General trash': 0.3383}, {'Paper': 0.6896}, {'Paper pack': 0.2177}, {'Metal': 0.3275}, {'Glass': 0.4211}, {'Plastic': 0.3353}, {'Styrofoam': 0.6048}, {'Plastic bag': 0.7804}, {'Battery': 0.0}, {'Clothing': 0.0481}]\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n",
      "Epoch [3/20], Step [25/327],                         Loss: 0.243, mIoU: 0.4306\n",
      "Epoch [3/20], Step [50/327],                         Loss: 0.4369, mIoU: 0.4317\n",
      "Epoch [3/20], Step [75/327],                         Loss: 0.3583, mIoU: 0.4367\n",
      "Epoch [3/20], Step [100/327],                         Loss: 0.2804, mIoU: 0.4485\n",
      "Epoch [3/20], Step [125/327],                         Loss: 0.3268, mIoU: 0.451\n",
      "Epoch [3/20], Step [150/327],                         Loss: 0.3578, mIoU: 0.4513\n",
      "Epoch [3/20], Step [175/327],                         Loss: 0.2154, mIoU: 0.4586\n",
      "Epoch [3/20], Step [200/327],                         Loss: 0.2334, mIoU: 0.4625\n",
      "Epoch [3/20], Step [225/327],                         Loss: 0.2319, mIoU: 0.4573\n",
      "Epoch [3/20], Step [250/327],                         Loss: 0.4966, mIoU: 0.4585\n",
      "Epoch [3/20], Step [275/327],                         Loss: 0.3251, mIoU: 0.4588\n",
      "Epoch [3/20], Step [300/327],                         Loss: 0.1762, mIoU: 0.4646\n",
      "Epoch [3/20], Step [325/327],                         Loss: 0.3137, mIoU: 0.4689\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 0.2897, Accuracy : 0.9137,                 mIoU: 0.5067\n",
      "IoU by class : [{'Backgroud': 0.9421}, {'General trash': 0.3995}, {'Paper': 0.7155}, {'Paper pack': 0.4104}, {'Metal': 0.3104}, {'Glass': 0.4761}, {'Plastic': 0.3598}, {'Styrofoam': 0.6087}, {'Plastic bag': 0.7917}, {'Battery': 0.0}, {'Clothing': 0.5597}]\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n",
      "Epoch [4/20], Step [25/327],                         Loss: 0.2197, mIoU: 0.5961\n",
      "Epoch [4/20], Step [50/327],                         Loss: 0.2469, mIoU: 0.5514\n",
      "Epoch [4/20], Step [75/327],                         Loss: 0.2266, mIoU: 0.5429\n",
      "Epoch [4/20], Step [100/327],                         Loss: 0.2477, mIoU: 0.5621\n",
      "Epoch [4/20], Step [125/327],                         Loss: 0.2529, mIoU: 0.5671\n",
      "Epoch [4/20], Step [150/327],                         Loss: 0.3229, mIoU: 0.5733\n",
      "Epoch [4/20], Step [175/327],                         Loss: 0.561, mIoU: 0.5754\n",
      "Epoch [4/20], Step [200/327],                         Loss: 0.1365, mIoU: 0.5754\n",
      "Epoch [4/20], Step [225/327],                         Loss: 0.156, mIoU: 0.5748\n",
      "Epoch [4/20], Step [250/327],                         Loss: 0.1286, mIoU: 0.5755\n",
      "Epoch [4/20], Step [275/327],                         Loss: 0.2595, mIoU: 0.5748\n",
      "Epoch [4/20], Step [300/327],                         Loss: 0.2324, mIoU: 0.5777\n",
      "Epoch [4/20], Step [325/327],                         Loss: 0.1723, mIoU: 0.5813\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 0.2804, Accuracy : 0.9171,                 mIoU: 0.5232\n",
      "IoU by class : [{'Backgroud': 0.9452}, {'General trash': 0.3705}, {'Paper': 0.7175}, {'Paper pack': 0.4172}, {'Metal': 0.3855}, {'Glass': 0.484}, {'Plastic': 0.3938}, {'Styrofoam': 0.6428}, {'Plastic bag': 0.8032}, {'Battery': 0.0}, {'Clothing': 0.5957}]\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n",
      "Epoch [5/20], Step [25/327],                         Loss: 0.0859, mIoU: 0.6628\n",
      "Epoch [5/20], Step [50/327],                         Loss: 0.1675, mIoU: 0.6165\n",
      "Epoch [5/20], Step [75/327],                         Loss: 0.1488, mIoU: 0.6261\n",
      "Epoch [5/20], Step [100/327],                         Loss: 0.1843, mIoU: 0.6308\n",
      "Epoch [5/20], Step [125/327],                         Loss: 0.1169, mIoU: 0.6463\n",
      "Epoch [5/20], Step [150/327],                         Loss: 0.1047, mIoU: 0.6545\n",
      "Epoch [5/20], Step [175/327],                         Loss: 0.222, mIoU: 0.6572\n",
      "Epoch [5/20], Step [200/327],                         Loss: 0.1405, mIoU: 0.6553\n",
      "Epoch [5/20], Step [225/327],                         Loss: 0.1465, mIoU: 0.6545\n",
      "Epoch [5/20], Step [250/327],                         Loss: 0.1354, mIoU: 0.653\n",
      "Epoch [5/20], Step [275/327],                         Loss: 0.1673, mIoU: 0.6491\n",
      "Epoch [5/20], Step [300/327],                         Loss: 0.2041, mIoU: 0.6526\n",
      "Epoch [5/20], Step [325/327],                         Loss: 0.0981, mIoU: 0.6508\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 0.2781, Accuracy : 0.922,                 mIoU: 0.5375\n",
      "IoU by class : [{'Backgroud': 0.9494}, {'General trash': 0.3335}, {'Paper': 0.7235}, {'Paper pack': 0.4502}, {'Metal': 0.4409}, {'Glass': 0.5228}, {'Plastic': 0.4502}, {'Styrofoam': 0.6536}, {'Plastic bag': 0.812}, {'Battery': 0.0}, {'Clothing': 0.576}]\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n",
      "Epoch [6/20], Step [25/327],                         Loss: 0.1792, mIoU: 0.6817\n",
      "Epoch [6/20], Step [50/327],                         Loss: 0.0999, mIoU: 0.6971\n",
      "Epoch [6/20], Step [75/327],                         Loss: 0.1174, mIoU: 0.6978\n",
      "Epoch [6/20], Step [100/327],                         Loss: 0.1368, mIoU: 0.6954\n",
      "Epoch [6/20], Step [125/327],                         Loss: 0.131, mIoU: 0.6986\n",
      "Epoch [6/20], Step [150/327],                         Loss: 0.1133, mIoU: 0.6994\n",
      "Epoch [6/20], Step [175/327],                         Loss: 0.1934, mIoU: 0.7007\n",
      "Epoch [6/20], Step [200/327],                         Loss: 0.1711, mIoU: 0.7005\n",
      "Epoch [6/20], Step [225/327],                         Loss: 0.1951, mIoU: 0.7057\n",
      "Epoch [6/20], Step [250/327],                         Loss: 0.1417, mIoU: 0.707\n",
      "Epoch [6/20], Step [275/327],                         Loss: 0.1124, mIoU: 0.7041\n",
      "Epoch [6/20], Step [300/327],                         Loss: 0.1278, mIoU: 0.7009\n",
      "Epoch [6/20], Step [325/327],                         Loss: 0.1646, mIoU: 0.7011\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 0.2729, Accuracy : 0.9239,                 mIoU: 0.5323\n",
      "IoU by class : [{'Backgroud': 0.9526}, {'General trash': 0.3317}, {'Paper': 0.7449}, {'Paper pack': 0.3868}, {'Metal': 0.3662}, {'Glass': 0.5242}, {'Plastic': 0.4377}, {'Styrofoam': 0.6784}, {'Plastic bag': 0.8129}, {'Battery': 0.0}, {'Clothing': 0.6199}]\n",
      "Epoch [7/20], Step [25/327],                         Loss: 0.2757, mIoU: 0.7705\n",
      "Epoch [7/20], Step [50/327],                         Loss: 0.1279, mIoU: 0.7062\n",
      "Epoch [7/20], Step [75/327],                         Loss: 0.1241, mIoU: 0.721\n",
      "Epoch [7/20], Step [100/327],                         Loss: 0.1846, mIoU: 0.7267\n",
      "Epoch [7/20], Step [125/327],                         Loss: 0.1568, mIoU: 0.7347\n",
      "Epoch [7/20], Step [150/327],                         Loss: 0.3718, mIoU: 0.7354\n",
      "Epoch [7/20], Step [175/327],                         Loss: 0.2041, mIoU: 0.7339\n",
      "Epoch [7/20], Step [200/327],                         Loss: 0.1307, mIoU: 0.7345\n",
      "Epoch [7/20], Step [225/327],                         Loss: 0.1088, mIoU: 0.7356\n",
      "Epoch [7/20], Step [250/327],                         Loss: 0.1007, mIoU: 0.7381\n",
      "Epoch [7/20], Step [275/327],                         Loss: 0.103, mIoU: 0.7373\n",
      "Epoch [7/20], Step [300/327],                         Loss: 0.0867, mIoU: 0.7389\n",
      "Epoch [7/20], Step [325/327],                         Loss: 0.0842, mIoU: 0.7427\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 0.2795, Accuracy : 0.9261,                 mIoU: 0.5523\n",
      "IoU by class : [{'Backgroud': 0.9538}, {'General trash': 0.3679}, {'Paper': 0.7532}, {'Paper pack': 0.4494}, {'Metal': 0.4575}, {'Glass': 0.5156}, {'Plastic': 0.4349}, {'Styrofoam': 0.6921}, {'Plastic bag': 0.8139}, {'Battery': 0.0}, {'Clothing': 0.6372}]\n",
      "Best performance at epoch: 7\n",
      "Save model in ./saved\n",
      "Epoch [8/20], Step [25/327],                         Loss: 0.1195, mIoU: 0.8262\n",
      "Epoch [8/20], Step [50/327],                         Loss: 0.0834, mIoU: 0.7564\n",
      "Epoch [8/20], Step [75/327],                         Loss: 0.0791, mIoU: 0.7576\n",
      "Epoch [8/20], Step [100/327],                         Loss: 0.0896, mIoU: 0.7548\n",
      "Epoch [8/20], Step [125/327],                         Loss: 0.0715, mIoU: 0.7558\n",
      "Epoch [8/20], Step [150/327],                         Loss: 0.1279, mIoU: 0.7527\n",
      "Epoch [8/20], Step [175/327],                         Loss: 0.0921, mIoU: 0.7519\n",
      "Epoch [8/20], Step [200/327],                         Loss: 0.1424, mIoU: 0.7513\n",
      "Epoch [8/20], Step [225/327],                         Loss: 0.1359, mIoU: 0.7534\n",
      "Epoch [8/20], Step [250/327],                         Loss: 0.3454, mIoU: 0.7497\n",
      "Epoch [8/20], Step [275/327],                         Loss: 0.0988, mIoU: 0.7505\n",
      "Epoch [8/20], Step [300/327],                         Loss: 0.0893, mIoU: 0.7506\n",
      "Epoch [8/20], Step [325/327],                         Loss: 0.1027, mIoU: 0.752\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 0.2659, Accuracy : 0.927,                 mIoU: 0.5518\n",
      "IoU by class : [{'Backgroud': 0.9549}, {'General trash': 0.3931}, {'Paper': 0.7533}, {'Paper pack': 0.4616}, {'Metal': 0.4082}, {'Glass': 0.5742}, {'Plastic': 0.429}, {'Styrofoam': 0.715}, {'Plastic bag': 0.8214}, {'Battery': 0.0}, {'Clothing': 0.5591}]\n",
      "Epoch [9/20], Step [25/327],                         Loss: 0.0809, mIoU: 0.7839\n",
      "Epoch [9/20], Step [50/327],                         Loss: 0.1204, mIoU: 0.7685\n",
      "Epoch [9/20], Step [75/327],                         Loss: 0.1048, mIoU: 0.7705\n",
      "Epoch [9/20], Step [100/327],                         Loss: 0.2896, mIoU: 0.7627\n",
      "Epoch [9/20], Step [125/327],                         Loss: 0.053, mIoU: 0.7715\n",
      "Epoch [9/20], Step [150/327],                         Loss: 0.0936, mIoU: 0.7724\n",
      "Epoch [9/20], Step [175/327],                         Loss: 0.1286, mIoU: 0.7721\n",
      "Epoch [9/20], Step [200/327],                         Loss: 0.134, mIoU: 0.7714\n",
      "Epoch [9/20], Step [225/327],                         Loss: 0.0919, mIoU: 0.7709\n",
      "Epoch [9/20], Step [250/327],                         Loss: 0.0875, mIoU: 0.7697\n",
      "Epoch [9/20], Step [275/327],                         Loss: 0.1852, mIoU: 0.7697\n",
      "Epoch [9/20], Step [300/327],                         Loss: 0.1387, mIoU: 0.7678\n",
      "Epoch [9/20], Step [325/327],                         Loss: 0.1566, mIoU: 0.7681\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 0.2722, Accuracy : 0.928,                 mIoU: 0.5838\n",
      "IoU by class : [{'Backgroud': 0.9553}, {'General trash': 0.3753}, {'Paper': 0.7485}, {'Paper pack': 0.4464}, {'Metal': 0.3745}, {'Glass': 0.5996}, {'Plastic': 0.4417}, {'Styrofoam': 0.7066}, {'Plastic bag': 0.8202}, {'Battery': 0.3314}, {'Clothing': 0.6229}]\n",
      "Best performance at epoch: 9\n",
      "Save model in ./saved\n",
      "Epoch [10/20], Step [25/327],                         Loss: 0.0777, mIoU: 0.7539\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 model 불러오기 (학습된 이후) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T05:17:34.490690Z",
     "start_time": "2021-10-04T05:17:34.339161Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = f'./saved/{model_name}.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "state_dict = checkpoint.state_dict()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model = model.to(device)\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_examples()` 시각화 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:16:19.666705Z",
     "start_time": "2021-10-04T06:16:19.657706Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(size, size)])\n",
    "    print('Start prediction.')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(tqdm(test_loader)):\n",
    "            \n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))['out']\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "                \n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:19:10.926207Z",
     "start_time": "2021-10-04T06:16:20.313208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(\"./submission/DLv3_res101_best_model(pretrained).csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "d36e052b391be8c28b05838ade06426769a29575d5fe21a7bc69c7dec0c04c06"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('segmentation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "394.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
